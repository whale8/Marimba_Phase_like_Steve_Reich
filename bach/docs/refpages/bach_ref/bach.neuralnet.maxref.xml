<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<?xml-stylesheet href="./_c74_ref.xsl" type="text/xsl"?>

<!--This file has been automatically generated by Doctor Max. DO NOT EDIT THIS FILE DIRECTLY.-->

<c74object name="bach.neuralnet" module="bach">

	<digest>
		Feed-forward or recurrent neural networks
	</digest>

	<description>
		Implements a feed-forwards or recurrent neural network with an arbitrary number of layers
	</description>

	<!--METADATA-->
	<metadatalist>
		<metadata name="author">bachproject</metadata>
		<metadata name="tag">bach</metadata>
		<metadata name="tag">bach objects</metadata>
		<metadata name="tag">bach llll</metadata>
	</metadatalist>

	<!--INLETS-->
	<inletlist>
		<inlet id="0" type="INLET_TYPE">
			<digest>
				The values of the input nodes, to be feeded to the network
			</digest>
			<description>
			</description>
		</inlet>
		<inlet id="1" type="INLET_TYPE">
			<digest>
				The training data in llll form
			</digest>
			<description>
			</description>
		</inlet>
	</inletlist>

	<!--OUTLETS-->
	<outletlist>
		<outlet id="0" type="INLET_TYPE">
			<digest>
				The output data for the incoming input data
			</digest>
			<description>
			</description>
		</outlet>
		<outlet id="1" type="INLET_TYPE">
			<digest>
				Dump outlet for the state of the network
			</digest>
			<description>
			</description>
		</outlet>
		<outlet id="2" type="INLET_TYPE">
			<digest>
				Advancement during training
			</digest>
			<description>
				The advancement is a floating point value between 0 (training starts) and 1 (training ends)
			</description>
		</outlet>
		<outlet id="3" type="INLET_TYPE">
			<digest>
				Output training results when training has ended
			</digest>
			<description>
				The output message is in the form
				<b>(<m>TS_accuracy</m> <m>GS_accuracy</m> <m>VS_accuracy</m>) (<m>TS_mse</m> <m>GS_mse</m> <m>VS_mse</m>)</b>
				where TS is the training set, GS is the generalization set, VS is the validation set,
				and mse is the mean squared error.
			</description>
		</outlet>
	</outletlist>

	<!--ARGUMENTS-->
	<objarglist>
		<objarg name="recurrent" optional="1" type="symbol">
			<digest>
				Recurrent
			</digest>
			<description>
				If the first argument is the symbole "recurrent", the neural network will be instantiated as a RNN (recurrent neural network),
				otherwise it will be a feed-forward neural network (default).
			</description>
		</objarg>
		<objarg name="in" optional="0" type="int">
			<digest>
				Number of input nodes
			</digest>
			<description>
				Sets the number of nodes in the input layer of the neural network.
			</description>
		</objarg>
		<objarg name="hidden" optional="0" type="list">
			<digest>
				Number of hidden nodes for each inner layer
			</digest>
			<description>
				Sets the number of nodes in each of the hidden layers of the neural network (the number of layers will match the number of integers)
			</description>
		</objarg>
		<objarg name="out" optional="0" type="int">
			<digest>
				Number of output nodes
			</digest>
			<description>
				Sets the number of nodes in the output layer of the neural network.
			</description>
		</objarg>
	</objarglist>

	<!--MESSAGES-->
	<methodlist>
		<method name="bang">
			<arglist />
			<digest>
				Train network with last used training data.
			</digest>
			<description>
				A bang in the second inlet will train the network with the last used training data.
			</description>
		</method>
		<method name="dump">
			<arglist />
			<digest>
				Dump network state
			</digest>
			<description>
				The <m>dump</m> message dumps the network state (model) from the second outlet. This is an llll having the form
				<b><m>NEURONS</m> <m>WEIGHTS</m></b>. The <m>NEURONS</m> llll is in the form <b>(<m>INPUT</m> <m>HIDDENLAYER1</m> <m>HIDDENLAYER2</m>... <m>OUTPUT</m>)</b>
				and contains the wrapped lists of the activation values of all neurons in each one of the layers.
				If the network is not recurrent, the <m>WEIGHTS</m> llll is in the form <b>(<m>InputToHidden1</m> <m>Hidden1ToHidden2</m> ... <m>LastHiddenLayerToOutput</m>)</b>
				where each one of the sub elements is indeed a matrix containing all the weights to pass from a given layer to the next one.
				If the network is recurrent, between any two elements of such lists are interleaved the weights for passing from a layer to itself, yielding the form
				<b>(<m>InputToHidden1</m> <m>Hidden1ToHidden1</m> <m>Hidden1ToHidden2</m> <m>Hidden2ToHidden2</m> ... <m>LastHiddenLayerToLastHiddenLayer</m> <m>LastHiddenLayerToOutput</m>)</b>
			</description>
		</method>
		<method name="llll">
			<arglist />
			<digest>
				Train or feed network
			</digest>
			<description>
				An <m>llll</m> in first inlet is considered as a list of input values to be fed to the network.
				The corresponding output values are output from the first outlet. An <m>llll</m> in second inlet is considered as a list of
				training items; each item must be in the form <b>((<m>input1</m> <m>input2</m>...) (<m>target1</m> <m>target2</m>...))</b>
				containing the inputs and the expected targets for such inputs. Once the training is over, a message containing the results is output
				through the third outlet (see outlet documentation for more information).
			</description>
		</method>
		<method name="read">
			<arg name="file_name" optional="1" type="symbol" />
			<digest>
				Read network state from disk
			</digest>
			<description>
				The specified file is read from disk and the llll it contains is considered to be the model (network state)
				The input model must be an llll of the same form as the one output by the <m>dump</m> message (see <m>dump</m> for more information).
			</description>
		</method>
		<method name="reset">
			<arg name="what" optional="1" type="symbol" />
			<digest>
				Reset network
			</digest>
			<description>
				The <m>reset</m> message in the second inlet resets the weights of the network to random values, the neurons activation values to 0, and
				resets the obtained training results.
				If the <m>reset</m> message is followed by one of the symbols <m>weights</m>, <m>neurons</m>, <m>results</m>, only the corresponding data is reset,
				while the others are preserved.
			</description>
		</method>
		<method name="setstate">
			<arglist />
			<digest>
				Set network state
			</digest>
			<description>
				An <m>llll</m> after the <m>setstate</m> message sets the state to the network to the input model.
				The input model must be an llll of the same form as the one output by the <m>dump</m> message (see <m>dump</m> for more information).
			</description>
		</method>
		<method name="stop">
			<arglist />
			<digest>
				Stop training
			</digest>
			<description>
				A <m>stop</m> message will cause the training to stop immediately.
			</description>
		</method>
		<method name="write">
			<arg name="file" optional="1" type="symbol" />
			<digest>
				Save network state in native format
			</digest>
			<description>
				A <m>write</m> message will save the network state in a file, in native format.
				If an argument is given, this is the filename (if the file is in the search path
				or in the same folder) or the full file path. If no argument is given, a dialog window pops up.
			</description>
		</method>
		<method name="writetxt">
			<arg name="file" optional="1" type="symbol" />
			<digest>
				Save network state in text format
			</digest>
			<description>
				A <m>writetxt</m> message will save the network state in a file, in readable text format.
				If an argument is given, this is the filename (if the file is in the search path
				or in the same folder) or the full file path. If no argument is given, a dialog window pops up.
			</description>
		</method>
	</methodlist>

	<!--ATTRIBUTES-->
	<attributelist>
		<attribute name="activationfunction" get="1" set="1" type="int" size="1">
			<digest>
				Activation Function
			</digest>
			<description>
				Chooses the activation function for each neuron.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Activation Function" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="enumindex" />
			</attributelist>
		</attribute>
		<attribute name="batchlearning" get="1" set="1" type="int" size="1">
			<digest>
				Use Batch Learning
			</digest>
			<description>
				Toggles the ability to operate in batch learning mode.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Use Batch Learning" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="onoff" />
			</attributelist>
		</attribute>
		<attribute name="bpttsteps" get="1" set="1" type="int" size="1">
			<digest>
				Backpropagation Through Time Memory
			</digest>
			<description>
				Sets the length of the history for backpropagation through time (only used by recurrent networks).
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Backpropagation Through Time Memory" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="classificationtolerance" get="1" set="1" type="float" size="1">
			<digest>
				Classification Tolerance
			</digest>
			<description>
				Sets the maximum tolerance allowed for validity check in classification problems.
				For instance, if <m>classificationtolerance</m> is 0.1 (default), an output value of 0.09 will be considered correct
				in case the target value should have been 0. This is used in order to determine whether a pattern is correctly classified
				in order to properly compute the accuracy ration (also see <m>desiredaccuracy</m>).
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Classification Tolerance" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="datasetmode" get="1" set="1" type="int" size="1">
			<digest>
				Dataset Mode
			</digest>
			<description>
				Sets the approach to building the dataset starting from the training data: <br />
				<m>Static</m>: The database is built  <br />
				<m>Growing</m>: The dat<br />
				<m>Selected</m>: Only show numbers when the corresponding point is selected<br />
				<m>Always</m>: Always show all the numbers
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Dataset" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Dataset Mode" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="enumindex" />
			</attributelist>
		</attribute>
		<attribute name="datasetsplit" get="1" set="1" type="float_array" size="0">
			<digest>
				Dataset Split
			</digest>
			<description>
				Sets the splitting ratios for generalization and validation segments of the dataset.
				For instance, <b>0.6 0.9</b> means that the 60% of the databased is used for training, the following 30%
				is used for generalization and the last 10% is used for validation
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Dataset" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Dataset Split" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="datasetstep" get="1" set="1" type="float" size="1">
			<digest>
				Dataset Step Size
			</digest>
			<description>
				Sets the ratio of the database representing the database growth (for Growing <m>datasetmode</m>) or
				the dataset shift (for Windowing <m>datasetmode</m>). This attribute is ignored with Static <m>datasetmode</m>.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Dataset" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Dataset Step Size" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="datasetwin" get="1" set="1" type="float" size="1">
			<digest>
				Dataset Window Size
			</digest>
			<description>
				Sets the size of the window for Windowing <m>datasetmode</m>, as a ratio on the whole training data.
				Must be bigger or equal to <m>datasetstep</m> size.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Dataset" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Dataset Window Size" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="desiredaccuracy" get="1" set="1" type="float" size="1">
			<digest>
				Desired Accuracy
			</digest>
			<description>
				Sets the desired accuracy for the training and generalization sets. The accuracy is the ratio between correctly classified
				patterns and whole training/generalization patterns. Once the training has reached
				the <m>desiredaccuracy</m> on both training and generalization sets, the training ends, even if <m>maxepochs</m> is not yet
				reached. Set this attribute to 0 (default) if you simply want to run the training in any case till <m>maxepochs</m> is reached (or till <m>desiredmse</m> is reached, if set).
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Desired Accuracy" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="desiredmse" get="1" set="1" type="float" size="1">
			<digest>
				Desired Mean Squared Error
			</digest>
			<description>
				Sets the desired mean squared error for the training and generalization sets. The mean squared error is
				is the average of the sum of the squared errors (desired – actual) for each pattern in the training/generalization set.
				Once the training has reached the <m>desiredmse</m> on both training and generalization sets, the training ends, even if <m>maxepochs</m> is not yet
				reached. Set this attribute to 0 (default) if you simply want to run the training in any case till <m>maxepochs</m> is reached (or till <m>desiredaccuracy</m> is reached, if set).
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Desired Mean Squared Error" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="embed" get="1" set="1" type="int" size="1">
			<digest>
				Save Trained Model With Patcher
			</digest>
			<description>
				When set to 1, the trained model is saved with the patcher
				and will be available, already charged, next time the patch is loaded.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Behavior" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Save Trained Model With Patcher" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="save" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="onoff" />
			</attributelist>
		</attribute>
		<attribute name="learningrate" get="1" set="1" type="float" size="1">
			<digest>
				Learning Rate
			</digest>
			<description>
				Sets the learning rate for the neural net, defaults to 0.001.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Learning Rate" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="maxepochs" get="1" set="1" type="int" size="1">
			<digest>
				Maximum Number of Training Epochs
			</digest>
			<description>
				Sets the maximum number of epochs to be run while training.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Maximum Number of Training Epochs" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="momentum" get="1" set="1" type="float" size="1">
			<digest>
				Momentum
			</digest>
			<description>
				Sets the momentum for the neural net, defaults to 0.9.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Training" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Momentum" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="out" get="1" set="1" type="symbol" size="1">
			<digest>
				Outlet Types
			</digest>
			<description>
				The <m>out</m> attribute is a sequence of characters identifying the outlet types (one character for each llll outlet). Characters can be one of the followings: <br />
				<m>n</m> (default): 'native' output: faster and more precise between bach objects, but unreadable by standard Max objects (an "bach.llll" message appears instead). <br />
				<m>t</m>: 'text' output: slower and limited in size, but readable by standard Max objects. A plain llll in text format is a generic Max message. <br />
				<m>x</m>: disabled output (nothing is output) <br />
				<m>p</m>: 'portal' output: only used by bach.portal to intercept the @out attribute specified while creating the abstraction (see bach.portal). <br />
				The native output is recommended, unless communication with standard Max messages is needed. Disabling unused outputs can be useful if extreme optimization of the patch is needed.
				<br /> <br />
				llllobj_class_add_check_attr(c, LLLL_OBJ_VANILLA);
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Behavior" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Outlet Types" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="text" />
			</attributelist>
		</attribute>
		<attribute name="parallel" get="1" set="1" type="int" size="1">
			<digest>
				Parallel
			</digest>
			<description>
				When set to 1, the training is performed in a separate thread.
				This means that the normal Max operation is not interrupted.
				The <m>parallel</m> attribute is highly experimental and can lead to serious problems,
				including data corruption and crashes.
				In general, it is essential that the patch is not saved or closed
				as long as the search is going on. Such operations are more than likely to crash Max.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Behavior" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Parallel" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="onoff" />
			</attributelist>
		</attribute>
		<attribute name="verbose" get="1" set="1" type="int" size="1">
			<digest>
				Verbose Training
			</digest>
			<description>
				Toggles the ability to verbosely print training information in the Max window (if verbose == 1) or from the
				last outlet, in the form <b>verbose <m>epoch</m> (<m>TSaccuracy</m> <m>TSmeansquarederror</m>) (<m>GSaccuracy</m> <m>GSmeansquarederror</m>)</b>.
				Some acronyms are used, namely: TS = Training Set, GS = Generalization Set, VS = Validation Set.
			</description>
			<attributelist>
				<attribute name="category" get="1" set="1" type="symbol" size="1" value="Behavior" />
				<attribute name="label" get="1" set="1" type="symbol" size="1" value="Verbose Training" />
				<attribute name="paint" get="1" set="1" type="int" size="1" value="1" />
				<attribute name="style" get="1" set="1" type="symbol" size="1" value="enumindex" />
			</attributelist>
		</attribute>
	</attributelist>

	<!--DISCUSSION-->
	<discussion>
		The training is achieved via feed forwarding and then backpropagation through gradient descent.
		The algorithm can operate both in stochastic and in batch mode.
		Even for recurrent neural networks, currently, only standard backpropagation is used (no backpropagation through time).
	</discussion>

	<!--SEEALSO-->
	<seealsolist>
		<seealso name="bach.constraints" />
	</seealsolist>

	<misc name = "Input">
		<entry name ="Inlet 1 (list/llll)">
			<description>
				The values of the input nodes, to be feeded to the network.
			</description>
		</entry>
		<entry name ="Inlet 2 (llll)">
			<description>
				The training data in llll form.
			</description>
		</entry>
	</misc>

	<misc name = "Output">
		<entry name ="Outlet 1 (llll)">
			<description>
				The output data for the incoming input data.
			</description>
		</entry>
		<entry name ="Outlet 2 (llll)">
			<description>
				Dump outlet for the state of the network.
			</description>
		</entry>
		<entry name ="Outlet 3 (float)">
			<description>
				Advancement during training.
				The advancement is a floating point value between 0 (training starts) and 1 (training ends)
			</description>
		</entry>
		<entry name ="Outlet 4 (llll)">
			<description>
				Output training results when training has ended.
				The output message is in the form
				<b>(<m>TS_accuracy</m> <m>GS_accuracy</m> <m>VS_accuracy</m>) (<m>TS_mse</m> <m>GS_mse</m> <m>VS_mse</m>)</b>
				where TS is the training set, GS is the generalization set, VS is the validation set,
				and mse is the mean squared error.
			</description>
		</entry>
	</misc>

	<misc name = "Discussion">
		<entry name ="More details">
			<description>
		The training is achieved via feed forwarding and then backpropagation through gradient descent.
		The algorithm can operate both in stochastic and in batch mode.
		Even for recurrent neural networks, currently, only standard backpropagation is used (no backpropagation through time).
			</description>
		</entry>
		<entry name ="Keywords">
			<description>
neural, neuron, network, net, perceptron, machine learning, backpropagation, feed, feedforward, recurrent, batch, stochastic, layer, train.
			</description>
		</entry>
	</misc>

</c74object>